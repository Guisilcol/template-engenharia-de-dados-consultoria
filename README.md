# Template de Engenharia de Dados - GCP

Template completo para projetos de Engenharia de Dados na Google Cloud Platform (GCP), utilizando Terraform para infraestrutura como cÃ³digo (IaC) e Cloud Run Jobs para processamento de dados.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Arquitetura](#-arquitetura)
- [PrÃ©-requisitos](#-prÃ©-requisitos)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [ConfiguraÃ§Ã£o Inicial](#-configuraÃ§Ã£o-inicial)
- [Deploy](#-deploy)
- [Uso](#-uso)
- [MÃ³dulos Terraform](#-mÃ³dulos-terraform)
- [Contribuindo](#-contribuindo)

## ğŸ¯ VisÃ£o Geral

Este template fornece uma estrutura completa e padronizada para implementar pipelines de dados no GCP:

- **Infraestrutura como CÃ³digo (IaC)** com Terraform
- **Arquitetura Lakehouse** (Bronze, Silver, Gold)
- **Cloud Run Jobs** para processamento de dados
- **Workflows** do GCP para orquestraÃ§Ã£o
- **CI/CD** automatizado para mÃºltiplos ambientes
- **BigQuery** para armazenamento e anÃ¡lise de dados
- **Cloud Storage** para data lake

## ğŸ—ï¸ Arquitetura

### Camadas de Dados (Lakehouse)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reception  â”‚â”€â”€â”€â”€â–¶â”‚   Bronze    â”‚â”€â”€â”€â”€â–¶â”‚   Silver    â”‚â”€â”€â”€â”€â–¶â”‚    Gold     â”‚
â”‚   Layer     â”‚     â”‚   Layer     â”‚     â”‚   Layer     â”‚     â”‚   Layer     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Raw Data         Dados Brutos      Dados Limpos      Dados Agregados
```

- **Reception**: Dados brutos de ingestÃ£o inicial
- **Bronze**: Dados sem tratamento, histÃ³rico completo
- **Silver**: Dados limpos e validados
- **Gold**: Dados agregados e prontos para consumo

### Componentes Principais

- **Cloud Storage**: Armazenamento de dados em formato Parquet
- **BigQuery**: Data warehouse para anÃ¡lise
- **Cloud Run Jobs**: Processamento de dados containerizado
- **Workflows**: OrquestraÃ§Ã£o de pipelines
- **Artifact Registry**: Registro de imagens Docker

## ğŸ”§ PrÃ©-requisitos

### Ferramentas NecessÃ¡rias

- [Google Cloud SDK (gcloud)](https://cloud.google.com/sdk/docs/install) - CLI do GCP
- [Terraform](https://www.terraform.io/downloads) >= 1.0
- [Docker](https://docs.docker.com/get-docker/) - Para build de containers
- [Git](https://git-scm.com/downloads) - Controle de versÃ£o
- [Python](https://www.python.org/downloads/) >= 3.9 (para desenvolvimento de jobs)

### PermissÃµes no GCP

O usuÃ¡rio ou service account precisa das seguintes permissÃµes:

- `roles/owner` ou conjunto de roles especÃ­ficas:
  - `roles/compute.admin`
  - `roles/storage.admin`
  - `roles/bigquery.admin`
  - `roles/run.admin`
  - `roles/workflows.admin`
  - `roles/artifactregistry.admin`
  - `roles/iam.serviceAccountAdmin`

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ 000_infrastructure/          # Infraestrutura Terraform
â”‚   â”œâ”€â”€ 000_locals.tf           # VariÃ¡veis locais
â”‚   â”œâ”€â”€ 000_variables.tf        # VariÃ¡veis de entrada
â”‚   â”œâ”€â”€ 001_providers.tf        # ConfiguraÃ§Ã£o de providers
â”‚   â”œâ”€â”€ 002_apis.tf             # APIs do GCP
â”‚   â”œâ”€â”€ 003_iam.tf              # Service accounts e permissÃµes
â”‚   â”œâ”€â”€ 004_artifactory.tf      # Artifact Registry
â”‚   â”œâ”€â”€ 005_cloudrun.tf         # Cloud Run Jobs
â”‚   â”œâ”€â”€ 006_bigquery.tf         # BigQuery (lakehouse)
â”‚   â”œâ”€â”€ 007_workflows.tf        # Workflows de orquestraÃ§Ã£o
â”‚   â”œâ”€â”€ inventories/            # ConfiguraÃ§Ãµes por ambiente
â”‚   â”‚   â”œâ”€â”€ backend/            # Backend do Terraform
â”‚   â”‚   â”‚   â”œâ”€â”€ dev.conf
â”‚   â”‚   â”‚   â””â”€â”€ prd.conf
â”‚   â”‚   â”œâ”€â”€ env/                # VariÃ¡veis de ambiente
â”‚   â”‚   â”‚   â”œâ”€â”€ dev.env
â”‚   â”‚   â”‚   â””â”€â”€ prd.env
â”‚   â”‚   â””â”€â”€ tfvars/             # VariÃ¡veis Terraform
â”‚   â”‚       â”œâ”€â”€ dev.tfvars
â”‚   â”‚       â””â”€â”€ prd.tfvars
â”‚   â””â”€â”€ modules/                # MÃ³dulos Terraform reutilizÃ¡veis
â”‚       â”œâ”€â”€ cloud_run_job/      # MÃ³dulo de Cloud Run Jobs
â”‚       â”œâ”€â”€ lakehouse/          # MÃ³dulo de Lakehouse (BigQuery + Storage)
â”‚       â””â”€â”€ workflow/           # MÃ³dulo de Workflows
â”‚
â”œâ”€â”€ 001_jobs/                   # Jobs de processamento de dados
â”‚   â”œâ”€â”€ Dockerfile              # Imagem Docker para Cloud Run
â”‚   â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”‚   â”œâ”€â”€ 000_sample/             # Job de exemplo
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â””â”€â”€ shared/                 # MÃ³dulos compartilhados
â”‚       â””â”€â”€ shared_module.py
â”‚
â”œâ”€â”€ 002_orquestration/          # Workflows de orquestraÃ§Ã£o
â”‚   â””â”€â”€ 001_sample_workflow.yaml
â”‚
â”œâ”€â”€ 999_cicd/                   # Scripts de CI/CD
â”‚   â”œâ”€â”€ 00_create_project.bash  # CriaÃ§Ã£o de projeto GCP
â”‚   â””â”€â”€ 01_deploy.bash          # Deploy automatizado
â”‚
â”œâ”€â”€ README.md                   # Este arquivo
â”œâ”€â”€ requirements.txt            # DependÃªncias do projeto
â””â”€â”€ TODO.md                     # Tarefas pendentes
```

## ğŸš€ ConfiguraÃ§Ã£o Inicial

### 1. Criar Projeto no GCP

Execute o script para criar um novo projeto GCP e configurar o billing:

```bash
bash 999_cicd/00_create_project.bash
```

Este script irÃ¡:
- Criar ou utilizar um projeto GCP existente
- Vincular uma conta de faturamento
- Criar um bucket para o state do Terraform

Crie ao menos 2 projetos: uma para ambiente de DEV e outro para PROD.

### 2. Configurar Ambientes

Crie os arquivos de configuraÃ§Ã£o para cada ambiente em `000_infrastructure/inventories/`:

#### Backend Configuration (`backend/<ambiente>.conf`)

```hcl
bucket = "bucket-terraform-tfstate-<project-id>"
prefix = "terraform.tfstate"
```

#### Environment Variables (`env/<ambiente>.env`)

```bash
TF_VAR_project_id=seu-project-id
TF_VAR_region=southamerica-east1
TF_VAR_artifact_registry_name=docker-repo
TF_VAR_artifact_image_name_to_cloud_run=cloud-run-image
```

#### Terraform Variables (`tfvars/<ambiente>.tfvars`)

```hcl
bronze_dataset_name   = "bronze_layer"
silver_dataset_name   = "silver_layer"
gold_dataset_name     = "gold_layer"
reception_bucket_name = "bucket-reception-layer-<project-id>"
bronze_bucket_name    = "bucket-bronze-layer-<project-id>"
silver_bucket_name    = "bucket-silver-layer-<project-id>"
gold_bucket_name      = "bucket-gold-layer-<project-id>"
```

### 3. AutenticaÃ§Ã£o no GCP

```bash
# Login no GCP
gcloud auth login

# Configurar projeto
gcloud config set project <seu-project-id>

# Autenticar Docker para Artifact Registry
gcloud auth configure-docker <regiao>-docker.pkg.dev
```

## ğŸš¢ Deploy

### Deploy Completo

Execute o script de deploy passando a branch e o ambiente:

```bash
bash 999_cicd/01_deploy.bash <branch-name> <ambiente>
```

Exemplo:
```bash
bash 999_cicd/01_deploy.bash dev dev
bash 999_cicd/01_deploy.bash main prd
```

O script irÃ¡:
1. Validar todas as dependÃªncias (git, terraform, gcloud, docker)
2. Baixar o cÃ³digo da branch especificada
3. Carregar as variÃ¡veis de ambiente
4. Fazer build e push da imagem Docker para o Artifact Registry
5. Executar `terraform init` e `terraform apply`

## ğŸ’» Uso

### Criar um Novo Job

1. Crie um novo diretÃ³rio em `001_jobs/`:

```bash
mkdir 001_jobs/002_meu_job
```

2. Crie o arquivo `main.py`:

```python
import os
from google.cloud import bigquery

def main():
    project_id = os.environ.get("PROJECT_ID")
    # Seu cÃ³digo aqui
    print(f"Executando job no projeto: {project_id}")

if __name__ == "__main__":
    main()
```

3. Adicione o job no Terraform (`000_infrastructure/005_cloudrun.tf`):

```hcl
module "meu_job" {
  source = "./modules/cloud_run_job"

  name                  = "meu-job"
  location              = var.region
  project_id            = var.project_id
  service_account_email = google_service_account.cloud_run_job_service_account.email
  artifact_image_path   = local.artifact_image_path
  args                  = ["python", "002_meu_job/main.py"]
  cpu_limit             = "2"
  memory_limit          = "1Gi"
  max_retries           = 1
  
  env_vars = {
    PROJECT_ID = var.project_id
  }
}
```

4. Execute o deploy:

```bash
bash 999_cicd/01_deploy.bash dev dev
```

### Criar um Novo Workflow

1. Crie o arquivo YAML em `002_orquestration/`:

```yaml
main:
  params: [args]
  steps:
  - init:
      assign:
      - project_id: ${project_id}
      - region: ${region}

  - run_job:
      call: googleapis.run.v1.namespaces.jobs.run
      args:
        name: $${"namespaces/" + project_id + "/jobs/" + "meu-job"}
        location: ${region}

  - finish:
      return: "Workflow completed"
```

2. Adicione o workflow no Terraform (`000_infrastructure/007_workflows.tf`):

```hcl
module "meu_workflow" {
  source = "./modules/workflow"

  name                  = "meu-workflow"
  location              = var.region
  project_id            = var.project_id
  service_account_email = google_service_account.cloud_run_job_service_account.email
  workflow_yaml_path    = "${path.module}/../002_orquestration/002_meu_workflow.yaml"

  template_vars = {
    project_id = var.project_id
    region     = var.region
  }

  description = "Meu workflow customizado"
  
  labels = {
    environment = "dev"
  }
}
```

## ğŸ§© MÃ³dulos Terraform

### Cloud Run Job

Cria um Cloud Run Job para processamento de dados.

**VariÃ¡veis principais:**
- `name`: Nome do job
- `artifact_image_path`: Caminho da imagem no Artifact Registry
- `args`: Argumentos de execuÃ§Ã£o
- `cpu_limit`: Limite de CPU (ex: "1", "2")
- `memory_limit`: Limite de memÃ³ria (ex: "512Mi", "1Gi")
- `env_vars`: VariÃ¡veis de ambiente

### Lakehouse

Cria a arquitetura de lakehouse com BigQuery e Cloud Storage.

**Componentes:**
- 4 buckets GCS (Reception, Bronze, Silver, Gold)
- 3 datasets BigQuery (Bronze, Silver, Gold)
- Tabelas de exemplo (managed e external)

### Workflow

Cria um Workflow do GCP para orquestraÃ§Ã£o de jobs.

**VariÃ¡veis principais:**
- `workflow_yaml_path`: Caminho do arquivo YAML
- `template_vars`: VariÃ¡veis a serem substituÃ­das no template
- `service_account_email`: Service account para execuÃ§Ã£o

## ğŸ› ï¸ Desenvolvimento

### Instalar DependÃªncias Locais

```bash
pip install -r requirements.txt
```

### Estrutura de DependÃªncias

- **BigQuery**: `google-cloud-bigquery`
- **Storage**: `google-cloud-storage`
- **Logging**: `google-cloud-logging`
- **Data Processing**: `pandas`, `numpy`, `pyarrow`
- **HTTP Requests**: `requests`

### Boas PrÃ¡ticas

1. **MÃ³dulos Compartilhados**: Coloque cÃ³digo reutilizÃ¡vel em `001_jobs/shared/`
2. **VariÃ¡veis de Ambiente**: Use variÃ¡veis de ambiente para configuraÃ§Ãµes
3. **Logging**: Use Cloud Logging para monitoramento
4. **IdempotÃªncia**: Garanta que seus jobs possam ser executados mÃºltiplas vezes
5. **Particionamento**: Use partiÃ§Ãµes no BigQuery para otimizar custos

## ğŸ¤ Contribuindo

1. Crie uma branch para sua feature: `git checkout -b feature/minha-feature`
2. FaÃ§a commit das suas mudanÃ§as: `git commit -m 'feat: adiciona nova feature'`
3. Push para a branch: `git push origin feature/minha-feature`
4. Abra um Pull Request

### ConvenÃ§Ã£o de Commits

Seguimos o padrÃ£o [Conventional Commits](https://www.conventionalcommits.org/):

- `feat`: Nova funcionalidade
- `fix`: CorreÃ§Ã£o de bug
- `docs`: MudanÃ§as na documentaÃ§Ã£o
- `refactor`: RefatoraÃ§Ã£o de cÃ³digo
- `test`: AdiÃ§Ã£o ou modificaÃ§Ã£o de testes
- `chore`: Tarefas de manutenÃ§Ã£o

## ğŸ“ LicenÃ§a

Este Ã© um template privado para uso em projetos de consultoria de engenharia de dados.